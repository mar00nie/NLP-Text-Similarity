{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk, re, string, os\n",
    "import gensim, spacy, glove\n",
    "import sent2vec\n",
    "\n",
    "from IPython.display import display\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy import linalg, mat, dot\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models.wrappers import FastText\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets\n",
    "\n",
    "sick = open(\"SICK.txt\", \"r\")\n",
    "msr1 = open(\"msr_train.txt\", \"r\")\n",
    "msr2 = open(\"msr_test.txt\", \"r\")\n",
    "#ppdb = open(\"ppdb-2.0-l-all\", \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data munging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_text_file(file):\n",
    "    \n",
    "    '''Reads lines in file and appends to a corpus list'''\n",
    "    \n",
    "    corpus = []\n",
    "    for i, line in enumerate(file):\n",
    "        if i==0:\n",
    "            continue\n",
    "        else:\n",
    "            corpus.append(line)\n",
    "    \n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_more_files(file1, file2=None):\n",
    "    \n",
    "    '''Reads lines in two files and appends them to a corpus list'''\n",
    "    \n",
    "    if file2==None:\n",
    "        corpus = read_text_file(file1)\n",
    "        \n",
    "    else:\n",
    "        corpus1 = read_text_file(file1)\n",
    "        corpus2 = read_text_file(file2)\n",
    "        corpus = corpus1 + corpus2\n",
    "    \n",
    "    print(len(corpus))\n",
    "    return corpus   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "#ppdb_corpus = read_more_files(ppdb)\n",
    "#ppdb_corpus\n",
    "\n",
    "./fasttext skipgram -input billwords_preprocessed.txt -dim 300 -thread 20 -minCount 5 -neg 5 -ws 5 -lr 0.025 -output modelft_sg\n",
    "\n",
    "./fasttext cbow -input billwords_preprocessed.txt -dim 300 -thread 20 -minCount 5 -neg 5 -ws 5 -lr 0.05 -output modelft_cbow\n",
    "\n",
    "./fasttext sent2vec -input billwords_preprocessed.txt -output modelsn_train -minCount 5 -dim 300 -epoch 5 -lr 0.2 -wordNgrams 1 -loss ns -neg 5 -thread 20\n",
    "\n",
    "./fasttext sent2vec -input billwords_preprocessed.txt -output modelsn_unibi -minCount 5 -dim 300 -epoch 5 -lr 0.2 -wordNgrams 2 -loss ns -neg 5 -thread 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_corpus = read_more_files(sick)\n",
    "sick_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "msr_corpus = read_more_files(msr1, msr2)\n",
    "msr_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sentences_scores(corpus):\n",
    "    \n",
    "    '''Creates a list of sentence pairs omitting punctuation and a list of similarity scores'''\n",
    "    \n",
    "    sentences, scores = [], []\n",
    "    for line in corpus:\n",
    "        words = line.split('\\t')\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            try:\n",
    "                int(word[0])\n",
    "                continue\n",
    "                \n",
    "            except:\n",
    "                sent1 = words[i].lower().strip()\n",
    "                sent2 = words[i+1].lower().strip()\n",
    "                \n",
    "                translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "                sent1 = sent1.translate(translator)\n",
    "                sent2 = sent2.translate(translator)\n",
    "                \n",
    "                sentences.append([sent1, sent2])\n",
    "                \n",
    "                if i==1:\n",
    "                    score = words[4]\n",
    "                    scores.append(float(score))\n",
    "                \n",
    "                elif i==3:\n",
    "                    score = words[0]\n",
    "                    scores.append(float(score))\n",
    "                \n",
    "                break\n",
    "            \n",
    "            else: continue\n",
    "        \n",
    "    return sentences, scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_sentences, sick_scores = extract_sentences_scores(sick_corpus)\n",
    "msr_sentences, msr_scores = extract_sentences_scores(msr_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "def stemming_words(sentences):\n",
    "    \n",
    "    '''Stems words in sentences using Porter Stemmer'''\n",
    "    \n",
    "    porter_stemmer = PorterStemmer()\n",
    "    \n",
    "    sentences_stem = []\n",
    "    for pair in sentences:\n",
    "        pair_new = []\n",
    "        for sent in pair:\n",
    "            sent_new = []\n",
    "            words = word_tokenize(sent)\n",
    "            for word in words:\n",
    "                sent_new.append(porter_stemmer.stem(word))\n",
    "            doc = ' '.join(sent_new)\n",
    "            pair_new.append(doc)\n",
    "        \n",
    "        sentences_stem.append(pair_new)\n",
    "    \n",
    "    return sentences_stem\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_sentences_stem = stemming_words(sick_sentences)\n",
    "msr_sentences_stem = stemming_words(msr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_sentences_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_sentences_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "\n",
    "    '''Gets POS tags from Wordnet'''\n",
    "    \n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return ''\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "def pos_tagging(sentences):\n",
    "    \n",
    "    '''Part-of-Speech tagging using Wordnet treebank tags'''\n",
    "    \n",
    "    sentences_pos = []\n",
    "    for pair in sentences:\n",
    "        pair_new = []\n",
    "        for sent in pair:\n",
    "            sent_new = []\n",
    "            words = word_tokenize(sent)\n",
    "            tagged = pos_tag(words)\n",
    "            for token, tag in tagged:\n",
    "                wntag = get_wordnet_pos(tag)\n",
    "                wordset = (token, wntag)\n",
    "                sent_new.append(wordset)\n",
    "            pair_new.append(sent_new) \n",
    "        sentences_pos.append(pair_new)\n",
    "    \n",
    "    return sentences_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_sentences_pos = pos_tagging(sick_sentences)\n",
    "msr_sentences_pos = pos_tagging(msr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_sentences_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_sentences_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatizing_words(sentences):\n",
    "    \n",
    "    '''Lemmatizes words in sentences that are POS-tagged'''\n",
    "    \n",
    "    sentences_pos = pos_tagging(sentences)\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    sentences_lemma = []\n",
    "    for pair in sentences_pos:\n",
    "        pair_new = []\n",
    "        for sent in pair:\n",
    "            sent_new = []\n",
    "            for wordset in sent:\n",
    "                try:\n",
    "                    sent_new.append(lemmatizer.lemmatize(wordset[0], pos=wordset[1]))\n",
    "                except:\n",
    "                    sent_new.append(wordset[0])\n",
    "                doc = ' '.join(sent_new)\n",
    "            pair_new.append(doc)\n",
    "        \n",
    "        sentences_lemma.append(pair_new)\n",
    "    \n",
    "    return sentences_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_sentences_lemma = lemmatizing_words(sick_sentences)\n",
    "msr_sentences_lemma = lemmatizing_words(msr_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_sentences_lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_sentences_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords = stopwords.words('english')\n",
    "    \n",
    "def remove_stopwords(sentences, stopwords):\n",
    "    \n",
    "    '''Removes English stopwords from sentences'''\n",
    "    \n",
    "    filtered_words = []\n",
    "    for pair in sentences:\n",
    "        pair_new = []\n",
    "        for sent in pair:\n",
    "            sent_new = []\n",
    "            words = word_tokenize(sent)\n",
    "            for word in words:\n",
    "                if word not in stopwords:\n",
    "                    sent_new.append(word)\n",
    "            doc = ' '.join(sent_new)\n",
    "            pair_new.append(doc)\n",
    "        \n",
    "        filtered_words.append(pair_new)\n",
    "    \n",
    "    return filtered_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_filtered = remove_stopwords(sick_sentences_lemma, stopwords)\n",
    "msr_filtered = remove_stopwords(msr_sentences_lemma, stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_scores_to_binary(scores_list, threshold):\n",
    "    \n",
    "    '''Converts a percentage accuracy score to either a 0 or a 1'''\n",
    "    \n",
    "    bin_scores = []\n",
    "    for score in scores_list:\n",
    "        if score < threshold:\n",
    "            bin_scores.append(0)\n",
    "        else:\n",
    "            bin_scores.append(1)\n",
    "    \n",
    "    return bin_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def run_bow_model(sentences):\n",
    "    \n",
    "    '''Creates BoW model and calculates cosine similarity of each sentence pair'''\n",
    "    \n",
    "    count_vec = CountVectorizer(stop_words=\"english\", analyzer='word', ngram_range=(1, 2), \n",
    "                               max_df=1.0, min_df=0.5, max_features=None)\n",
    "    \n",
    "    bow_matrix, bow_scores = [], []\n",
    "    for pair in sentences:\n",
    "        bow_model = count_vec.fit_transform(pair).toarray()\n",
    "        bow_matrix.append(bow_model)\n",
    "\n",
    "        cos_sim = cosine_similarity(bow_model[0,:].reshape(1,-1), bow_model[1,:].reshape(1, -1))\n",
    "        bow_scores.append(float(cos_sim))\n",
    "        \n",
    "    \n",
    "    return bow_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_bow_scores = run_bow_model(sick_sentences)\n",
    "msr_bow_scores = run_bow_model(msr_sentences)\n",
    "\n",
    "sick_bow_stem_scores = run_bow_model(sick_sentences_stem)\n",
    "msr_bow_stem_scores = run_bow_model(msr_sentences_stem)\n",
    "\n",
    "sick_bow_lem_scores = run_bow_model(sick_sentences_lemma)\n",
    "msr_bow_lem_scores = run_bow_model(msr_sentences_lemma)\n",
    "\n",
    "sick_bow_stw_scores = run_bow_model(sick_filtered)\n",
    "msr_bow_stw_scores = run_bow_model(msr_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_bow_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msr_bow_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_bow = pd.DataFrame({'Relatedness': sick_scores, 'BoW': sick_bow_scores, 'BoW_stem': sick_bow_stem_scores,\n",
    "                           'BoW_lemma': sick_bow_lem_scores, 'BoW_filt': sick_bow_stw_scores})\n",
    "df_sick_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson's correlation of scores\n",
    "df_sick_bow.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's correlation of scores\n",
    "df_sick_bow.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scores to binary\n",
    "\n",
    "msr_bow_binscores = convert_scores_to_binary(msr_bow_scores, 0.7)\n",
    "msr_bow_stem_binscores = convert_scores_to_binary(msr_bow_stem_scores, 0.7)\n",
    "msr_bow_lem_binscores = convert_scores_to_binary(msr_bow_lem_scores, 0.7)\n",
    "msr_bow_stw_binscores = convert_scores_to_binary(msr_bow_stw_scores, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msr_bow = pd.DataFrame({'Similarity': msr_scores, 'BoW': msr_bow_binscores, 'BoW_stem': msr_bow_stem_binscores,\n",
    "                           'BoW_lemma': msr_bow_lem_binscores, 'BoW_filt': msr_bow_stw_binscores})\n",
    "df_msr_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix of BoW scores\n",
    "\n",
    "#plt.figure(figsize=(6, 4))\n",
    "\n",
    "msr_bow_cf = pd.DataFrame(confusion_matrix(df_msr_bow['Similarity'], df_msr_bow['BoW']),  \n",
    "                      columns=['BoW 0', 'BoW 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_bow_acc = accuracy_score(df_msr_bow['Similarity'], df_msr_bow['BoW'])\n",
    "\n",
    "msr_bow_stem_cf = pd.DataFrame(confusion_matrix(df_msr_bow['Similarity'], df_msr_bow['BoW_stem']),  \n",
    "                      columns=['BoW_stem 0', 'BoW_stem 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_bow_stem_acc = accuracy_score(df_msr_bow['Similarity'], df_msr_bow['BoW_stem'])\n",
    "\n",
    "msr_bow_lem_cf = pd.DataFrame(confusion_matrix(df_msr_bow['Similarity'], df_msr_bow['BoW_lemma']),  \n",
    "                      columns=['BoW_lemma 0', 'BoW_lemma 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_bow_lem_acc = accuracy_score(df_msr_bow['Similarity'], df_msr_bow['BoW_lemma'])\n",
    "\n",
    "msr_bow_stw_cf = pd.DataFrame(confusion_matrix(df_msr_bow['Similarity'], df_msr_bow['BoW_filt']),  \n",
    "                      columns=['BoW_filt 0', 'BoW_filt 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_bow_stw_acc = accuracy_score(df_msr_bow['Similarity'], df_msr_bow['BoW_filt'])\n",
    "\n",
    "#sns.heatmap(msr_bow_cf, annot=True, cmap='Blues')\n",
    "#plt.show()\n",
    "display(msr_bow_cf)\n",
    "print('Accuracy:', msr_bow_acc)\n",
    "display(msr_bow_stem_cf)\n",
    "print('Accuracy:', msr_bow_stem_acc)\n",
    "display(msr_bow_lem_cf)\n",
    "print('Accuracy:', msr_bow_lem_acc)\n",
    "display(msr_bow_stw_cf)\n",
    "print('Accuracy:', msr_bow_stw_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve and AUC score\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "# ROC curve; AUC score\n",
    "fpr, tpr, thresholds = roc_curve(msr_scores, msr_bow_stw_scores, pos_label=1)\n",
    "roc_auc_dt = auc(fpr, tpr)\n",
    "\n",
    "plt.title('ROC Curve - BoW_filt')\n",
    "plt.plot(fpr, tpr, 'b',\n",
    "label='AUC = %0.4f'% roc_auc_dt)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.1,1.2])\n",
    "plt.ylim([-0.1,1.2])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def run_tfidf_model(sentences):\n",
    "    \n",
    "    '''Creates TF-IDF model and calculates cosine similarity of each sentence pair'''\n",
    "    \n",
    "    tfidf_vec = TfidfVectorizer(stop_words=\"english\", analyzer='word', ngram_range=(1, 2), \n",
    "                               max_df=1.0, min_df=0.5, max_features=None)\n",
    "    \n",
    "    tfidf_matrix, tfidf_scores = [], []\n",
    "    for pair in sentences:\n",
    "        tfidf_model = tfidf_vec.fit_transform(pair).toarray()\n",
    "        tfidf_matrix.append(tfidf_model)\n",
    "\n",
    "        cos_sim = cosine_similarity(tfidf_model[0,:].reshape(1,-1), tfidf_model[1,:].reshape(1, -1))\n",
    "        tfidf_scores.append(float(cos_sim))\n",
    "           \n",
    "    return tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_tfidf_scores = run_tfidf_model(sick_sentences)\n",
    "msr_tfidf_scores = run_tfidf_model(msr_sentences)\n",
    "\n",
    "sick_tfidf_stem_scores = run_tfidf_model(sick_sentences_stem)\n",
    "msr_tfidf_stem_scores = run_tfidf_model(msr_sentences_stem)\n",
    "\n",
    "sick_tfidf_lem_scores = run_tfidf_model(sick_sentences_lemma)\n",
    "msr_tfidf_lem_scores = run_tfidf_model(msr_sentences_lemma)\n",
    "\n",
    "sick_tfidf_stw_scores = run_tfidf_model(sick_filtered)\n",
    "msr_tfidf_stw_scores = run_tfidf_model(msr_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_tfidf_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_tfidf = pd.DataFrame({'Relatedness': sick_scores, 'TF-IDF': sick_tfidf_scores, \n",
    "                              'TF-IDF_stem': sick_tfidf_stem_scores, 'TF-IDF_lemma': sick_tfidf_lem_scores, \n",
    "                              'TF-IDF_filt': sick_tfidf_stw_scores})\n",
    "df_sick_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pearson's correlation of scores\n",
    "df_sick_tfidf.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spearman's correlation of scores\n",
    "df_sick_tfidf.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scores to binary\n",
    "\n",
    "msr_tfidf_binscores = convert_scores_to_binary(msr_tfidf_scores, 0.7)\n",
    "msr_tfidf_stem_binscores = convert_scores_to_binary(msr_tfidf_stem_scores, 0.7)\n",
    "msr_tfidf_lem_binscores = convert_scores_to_binary(msr_tfidf_lem_scores, 0.7)\n",
    "msr_tfidf_stw_binscores = convert_scores_to_binary(msr_tfidf_stw_scores, 0.7)\n",
    "\n",
    "msr_scores = convert_scores_to_binary(msr_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msr_tfidf = pd.DataFrame({'Similarity': msr_scores, 'TF-IDF': msr_tfidf_binscores, \n",
    "                             'TF-IDF_stem': msr_tfidf_stem_binscores, 'TF-IDF_lemma': msr_tfidf_lem_binscores, \n",
    "                             'TF-IDF_filt': msr_tfidf_stw_binscores})\n",
    "df_msr_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of TF-IDF scores\n",
    "\n",
    "#plt.figure(figsize=(6, 4))\n",
    "\n",
    "msr_tfidf_cf = pd.DataFrame(confusion_matrix(df_msr_tfidf['Similarity'], df_msr_tfidf['TF-IDF']),  \n",
    "                      columns=['TF-IDF 0', 'TF-IDF 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_tfidf_acc = accuracy_score(df_msr_tfidf['Similarity'], df_msr_tfidf['TF-IDF'])\n",
    "\n",
    "msr_tfidf_stem_cf = pd.DataFrame(confusion_matrix(df_msr_tfidf['Similarity'], df_msr_tfidf['TF-IDF_stem']),  \n",
    "                      columns=['TF-IDF_stem 0', 'TF-IDF_stem 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_tfidf_stem_acc = accuracy_score(df_msr_tfidf['Similarity'], df_msr_tfidf['TF-IDF_stem'])\n",
    "\n",
    "msr_tfidf_lem_cf = pd.DataFrame(confusion_matrix(df_msr_tfidf['Similarity'], df_msr_tfidf['TF-IDF_lemma']),  \n",
    "                      columns=['TF-IDF_lemma 0', 'TF-IDF_lemma 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_tfidf_lem_acc = accuracy_score(df_msr_tfidf['Similarity'], df_msr_tfidf['TF-IDF_lemma'])\n",
    "\n",
    "msr_tfidf_stw_cf = pd.DataFrame(confusion_matrix(df_msr_tfidf['Similarity'], df_msr_tfidf['TF-IDF_filt']),  \n",
    "                      columns=['TF-IDF_filt 0', 'TF-IDF_filt 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_tfidf_stw_acc = accuracy_score(df_msr_tfidf['Similarity'], df_msr_tfidf['TF-IDF_filt'])\n",
    "\n",
    "#sns.heatmap(msr_tfidf_cf, annot=True, cmap='Blues')\n",
    "#plt.show()\n",
    "display(msr_tfidf_cf)\n",
    "print('Accuracy:', msr_tfidf_acc)\n",
    "display(msr_tfidf_stem_cf)\n",
    "print('Accuracy:', msr_tfidf_stem_acc)\n",
    "display(msr_tfidf_lem_cf)\n",
    "print('Accuracy:', msr_tfidf_lem_acc)\n",
    "display(msr_tfidf_stw_cf)\n",
    "print('Accuracy:', msr_tfidf_stw_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model\n",
    "\n",
    "modelwd_pret = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelwd_pret['dog']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelwd_pret.most_similar(positive=['yes', 'yeah'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelwd_pret.similarity('good', 'great')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_words_list(sentences):\n",
    "    \n",
    "    '''Creates a list with separate words in each sentence'''\n",
    "    \n",
    "    words_list = []\n",
    "\n",
    "    for pair in sentences:\n",
    "        pair_new = []\n",
    "        for sent in pair:\n",
    "            words = sent.split()\n",
    "            pair_new.append(words)\n",
    "\n",
    "        words_list.append(pair_new)\n",
    "    \n",
    "    return words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_words_list = create_words_list(sick_filtered)\n",
    "msr_words_list = create_words_list(msr_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def run_gensim_model(words_list, model):\n",
    "    \n",
    "    '''Runs Word2Vec and GloVe models in gensim and computes cosine similarity scores for each sentence pair'''\n",
    "    \n",
    "    matrix_list, scores = [], []\n",
    "\n",
    "    for pair in words_list:\n",
    "        sum_list = []\n",
    "        for sent in pair:\n",
    "            embeddings_list = []\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    embeddings_list.append(model[word])\n",
    "                except:\n",
    "                    embeddings_list.append(np.array(0))\n",
    "            sum_list.append(sum(embeddings_list)/len(embeddings_list))\n",
    "    \n",
    "        matrix_list.append(sum_list)\n",
    "        cos_sim = cosine_similarity(sum_list[0].reshape(1, -1), sum_list[1].reshape(1, -1))\n",
    "        #cos_sim = dot(sum_list[0], sum_list[1].T)/(linalg.norm(sum_list[0])*linalg.norm(sum_list[1]))\n",
    "        scores.append(float(cos_sim))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_wdpret_scores = run_gensim_model(sick_words_list, modelwd_pret)\n",
    "msr_wdpret_scores = run_gensim_model(msr_words_list, modelwd_pret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_wdpret_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_wdpret_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_spacy_model(words_list, model):\n",
    "    \n",
    "    '''Runs Word2Vec and GloVe models in SpaCy and computes cosine similarity scores for each sentence pair'''\n",
    "    \n",
    "    scores = []\n",
    "    for pair in words_list:\n",
    "        token1 = model(pair[0])\n",
    "        token2 = model(pair[1])\n",
    "        scores.append(token1.similarity(token2))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelwd_spacy = spacy.load(\"en\", vectors=\"GoogleNews-vectors-negative300.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_wdspacy_scores = run_spacy_model(sick_filtered, modelwd_spacy)\n",
    "msr_wdspacy_scores = run_spacy_model(msr_filtered, modelwd_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_wdspacy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_wdspacy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save UTF-8 version of text for training\n",
    "\n",
    "billwords_raw = open(\"/project/1-billion-word-language-modeling-benchmark-r13output.tar\", \"r\", encoding=\"ISO-8859-1\")\n",
    "billwords_clean = open('/project/billion_words_dataset/billwords_preprocessed.txt', 'w')\n",
    "\n",
    "for i, line in enumerate(billwords_raw):\n",
    "    if i==0:\n",
    "        pos = line.find(\"While\")\n",
    "        line = line[pos:]\n",
    "        \n",
    "    billwords_clean.write(line)                \n",
    "\n",
    "billwords_raw.close()\n",
    "billwords_clean.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySentences(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "            for line in open(os.path.join(self.dirname, fname)):\n",
    "                line = line.translate(translator)\n",
    "                line = line.lower().strip()\n",
    "                yield line.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign documents to directory of dataset for training\n",
    "\n",
    "documents = MySentences(\"/project/billion_words_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train CBOW model\n",
    "\n",
    "modelwd_cbow = Word2Vec(documents, size=300, window=5, min_count=5, workers=20, negative=5, sg=0)\n",
    "#modelwd_train.train(documents, total_examples=len(documents), epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Skip-Gram model\n",
    "\n",
    "modelwd_sg = Word2Vec(documents, size=300, window=5, min_count=5, workers=20, negative=5, sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save CBOW model\n",
    "\n",
    "modelwd_cbow.wv.save_word2vec_format('modelwd_cbow.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Skip-Gram model\n",
    "\n",
    "modelwd_sg.wv.save_word2vec_format('modelwd_sg.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "modelwd_cbow.wv.most_similar(positive='good', negative='bad')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelwd_cbow.wv.similarity('person', 'people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementatin\n",
    "\n",
    "sick_wdcbow_scores = run_gensim_model(sick_words_list, modelwd_cbow)\n",
    "msr_wdcbow_scores = run_gensim_model(msr_words_list, modelwd_cbow)\n",
    "\n",
    "sick_wdsg_scores = run_gensim_model(sick_words_list, modelwd_sg)\n",
    "msr_wdsg_scores = run_gensim_model(msr_words_list, modelwd_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_wdcbow_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_wdsg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_w2v = pd.DataFrame({'Relatedness': sick_scores, 'Word2Vec_gensim': sick_wdpret_scores, \n",
    "                            'Word2Vec_spacy': sick_wdspacy_scores, 'Word2Vec_cbow': sick_wdcbow_scores,\n",
    "                            'Word2Vec_sg': sick_wdsg_scores})\n",
    "df_sick_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_w2v.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_w2v.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scores to binary\n",
    "\n",
    "msr_wdpret_binscores = convert_scores_to_binary(msr_wdpret_scores, 0.7)\n",
    "msr_wdspacy_binscores = convert_scores_to_binary(msr_wdspacy_scores, 0.7)\n",
    "msr_wdcbow_binscores = convert_scores_to_binary(msr_wdcbow_scores, 0.7)\n",
    "msr_wdsg_binscores = convert_scores_to_binary(msr_wdsg_scores, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msr_w2v = pd.DataFrame({'Similarity': msr_scores, 'Word2Vec_gensim': msr_wdpret_binscores, \n",
    "                           'Word2Vec_spacy': msr_wdspacy_binscores, 'Word2Vec_cbow': msr_wdcbow_binscores,\n",
    "                           'Word2Vec_sg': msr_wdsg_binscores})\n",
    "df_msr_w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of Word2Vec scores\n",
    "\n",
    "#plt.figure(figsize=(6, 4))\n",
    "\n",
    "msr_wdpret_cf = pd.DataFrame(confusion_matrix(df_msr_w2v['Similarity'], df_msr_w2v['Word2Vec_gensim']),  \n",
    "                      columns=['W2V_gensim 0', 'W2V_gensim 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_wdpret_acc = accuracy_score(df_msr_w2v['Similarity'], df_msr_w2v['Word2Vec_gensim'])\n",
    "\n",
    "msr_wdspacy_cf = pd.DataFrame(confusion_matrix(df_msr_w2v['Similarity'], df_msr_w2v['Word2Vec_spacy']),  \n",
    "                      columns=['W2V_spacy 0', 'W2V_spacy 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_wdspacy_acc = accuracy_score(df_msr_w2v['Similarity'], df_msr_w2v['Word2Vec_spacy'])\n",
    "\n",
    "msr_wdcbow_cf = pd.DataFrame(confusion_matrix(df_msr_w2v['Similarity'], df_msr_w2v['Word2Vec_cbow']),  \n",
    "                      columns=['W2V_cbow 0', 'W2V_cbow 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_wdcbow_acc = accuracy_score(df_msr_w2v['Similarity'], df_msr_w2v['Word2Vec_cbow'])\n",
    "\n",
    "msr_wdsg_cf = pd.DataFrame(confusion_matrix(df_msr_w2v['Similarity'], df_msr_w2v['Word2Vec_sg']),  \n",
    "                      columns=['W2V_sg 0', 'W2V_sg 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_wdsg_acc = accuracy_score(df_msr_w2v['Similarity'], df_msr_w2v['Word2Vec_sg'])\n",
    "\n",
    "#sns.heatmap(msr_wdpret_cf, annot=True, cmap='Blues')\n",
    "#plt.show()\n",
    "display(msr_wdpret_cf)\n",
    "print('Accuracy:', msr_wdpret_acc)\n",
    "display(msr_wdspacy_cf)\n",
    "print('Accuracy:', msr_wdspacy_acc)\n",
    "display(msr_wdcbow_cf)\n",
    "print('Accuracy:', msr_wdcbow_acc)\n",
    "display(msr_wdsg_cf)\n",
    "print('Accuracy:', msr_wdsg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pretrained corpus of GloVe vectors\n",
    "\n",
    "glove2word2vec(glove_input_file=\"glove.6B.300d.txt\", word2vec_output_file=\"gensim_glove_vectors.txt\")\n",
    "\n",
    "modelgl_pret = KeyedVectors.load_word2vec_format('gensim_glove_vectors.txt', binary=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_glpret_scores = run_gensim_model(sick_words_list, modelgl_pret)\n",
    "msr_glpret_scores = run_gensim_model(msr_words_list, modelgl_pret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sick_glpret_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_glpret_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#modelgl_spacy = spacy.load(\"en_vectors_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelgl_spacy = spacy.load(\"en\", vectors=\"glove.6B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_glspacy_scores = run_spacy_model(sick_filtered, modelgl_spacy)\n",
    "msr_glspacy_scores = run_spacy_model(msr_filtered, modelgl_spacy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sick_glspacy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_glspacy_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glove import Corpus, Glove\n",
    "\n",
    "def run_glove_model(words_list, model):\n",
    "    \n",
    "    '''Runs GloVe model in glove and computes cosine similarity scores for each sentence pair'''\n",
    "    \n",
    "    matrix_list, scores = [], []\n",
    "\n",
    "    for pair in words_list:\n",
    "        sum_list = []\n",
    "        for sent in pair:\n",
    "            embeddings_list = []\n",
    "            for word in sent:\n",
    "                try:\n",
    "                    embeddings_list.append(model.word_vectors[model.dictionary[word]])\n",
    "                except:\n",
    "                    embeddings_list.append(np.array(0))\n",
    "            sum_list.append(sum(embeddings_list)/len(embeddings_list))\n",
    "    \n",
    "        matrix_list.append(sum_list)\n",
    "        cos_sim = cosine_similarity(sum_list[0].reshape(1, -1), sum_list[1].reshape(1, -1))\n",
    "        #cos_sim = dot(sum_list[0], sum_list[1].T)/(linalg.norm(sum_list[0])*linalg.norm(sum_list[1]))\n",
    "        scores.append(float(cos_sim))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GloVe model using dataset assigned to documents\n",
    "\n",
    "documents = MySentences(\"/project/billion_words_dataset\")\n",
    "\n",
    "corpus = Corpus()\n",
    "\n",
    "corpus.fit(documents, window=5)\n",
    " \n",
    "modelgl_train = Glove(no_components=300, learning_rate=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train model with epochs\n",
    "\n",
    "modelgl_train.fit(corpus.matrix, epochs=5, no_threads=20, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add word embeddings to corpus dictionary\n",
    "\n",
    "modelgl_train.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelgl_train.save('modelgl_train.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained file from demo.sh\n",
    "\n",
    "glove2word2vec(glove_input_file=\"/project/GloVe/modelgl_train.txt\", word2vec_output_file=\"modelgl_vectors.txt\")\n",
    "\n",
    "# Load pretrained corpus of GloVe vectors\n",
    "modelgl_train = KeyedVectors.load_word2vec_format('modelgl_vectors.txt', binary=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelgl_train.most_similar('frog')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelgl_train.word_vectors[modelgl_train.dictionary['frog']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_gltrain_scores = run_gensim_model(sick_words_list, modelgl_train)\n",
    "msr_gltrain_scores = run_gensim_model(msr_words_list, modelgl_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_gltrain_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_gltrain_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scores to binary\n",
    "\n",
    "msr_glpret_binscores = convert_scores_to_binary(msr_glpret_scores, 0.7)\n",
    "msr_glspacy_binscores = convert_scores_to_binary(msr_glspacy_scores, 0.7)\n",
    "msr_gltrain_binscores = convert_scores_to_binary(msr_gltrain_scores, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_glv = pd.DataFrame({'Relatedness': sick_scores, 'GloVe_gensim': sick_glpret_scores, \n",
    "                              'GloVe_spacy': sick_glspacy_scores, 'GloVe_train': sick_gltrain_scores})\n",
    "df_sick_glv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_glv.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_glv.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msr_glv = pd.DataFrame({'Similarity': msr_scores, 'GloVe_gensim': msr_glpret_binscores, \n",
    "                              'GloVe_spacy': msr_glspacy_binscores, 'GloVe_train': msr_gltrain_binscores})\n",
    "df_msr_glv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Confusion matrix of GloVe scores\n",
    "\n",
    "#plt.figure(figsize=(6, 4))\n",
    "\n",
    "msr_glpret_cf = pd.DataFrame(confusion_matrix(df_msr_glv['Similarity'], df_msr_glv['GloVe_gensim']),  \n",
    "                      columns=['GloVe_gensim 0', 'GloVe_gensim 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_glpret_acc = accuracy_score(df_msr_glv['Similarity'], df_msr_glv['GloVe_gensim'])\n",
    "\n",
    "msr_glspacy_cf = pd.DataFrame(confusion_matrix(df_msr_glv['Similarity'], df_msr_glv['GloVe_spacy']),  \n",
    "                      columns=['GloVe_spacy 0', 'GloVe_spacy 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_glspacy_acc = accuracy_score(df_msr_glv['Similarity'], df_msr_glv['GloVe_spacy'])\n",
    "\n",
    "msr_gltrain_cf = pd.DataFrame(confusion_matrix(df_msr_glv['Similarity'], df_msr_glv['GloVe_train']),  \n",
    "                      columns=['GloVe_train 0', 'GloVe_train 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_gltrain_acc = accuracy_score(df_msr_glv['Similarity'], df_msr_glv['GloVe_train'])\n",
    "\n",
    "#sns.heatmap(msr_glpret_cf, annot=True, cmap='Blues')\n",
    "#plt.show()\n",
    "display(msr_glpret_cf)\n",
    "print('Accuracy:', msr_glpret_acc)\n",
    "display(msr_glspacy_cf)\n",
    "print('Accuracy:', msr_glspacy_acc)\n",
    "display(msr_gltrain_cf)\n",
    "print('Accuracy:', msr_gltrain_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained Skip-Gram word embeddings file\n",
    "\n",
    "modelft_sg = FastText.load_fasttext_format('/project/fastText/modelft_sg.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained CBOW word embeddings file\n",
    "\n",
    "modelft_cbow = FastText.load_fasttext_format('/project/fastText/modelft_cbow.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelft_cbow.most_similar('teacher')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelft_cbow.similarity('teacher', 'lecturer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_ftsg_scores = run_gensim_model(sick_words_list, modelft_sg)\n",
    "sick_ftcbow_scores = run_gensim_model(sick_words_list, modelft_cbow)\n",
    "\n",
    "msr_ftsg_scores = run_gensim_model(msr_words_list, modelft_sg)\n",
    "msr_ftcbow_scores = run_gensim_model(msr_words_list, modelft_cbow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_ftcbow_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_ftsg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scores to binary\n",
    "\n",
    "msr_ftsg_binscores = convert_scores_to_binary(msr_ftsg_scores, 0.7)\n",
    "msr_ftcbow_binscores = convert_scores_to_binary(msr_ftcbow_scores, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_ft = pd.DataFrame({'Relatedness': sick_scores, 'fastText_cbow': sick_ftcbow_scores,\n",
    "                            'fastText_sg': sick_ftsg_scores})\n",
    "df_sick_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_ft.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_ft.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msr_ft = pd.DataFrame({'Similarity': msr_scores, 'fastText_cbow': msr_ftcbow_binscores,\n",
    "                           'fastText_sg': msr_ftsg_binscores})\n",
    "df_msr_ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of fastText scores\n",
    "\n",
    "#plt.figure(figsize=(6, 4))\n",
    "\n",
    "msr_ftcbow_cf = pd.DataFrame(confusion_matrix(df_msr_ft['Similarity'], df_msr_ft['fastText_cbow']),  \n",
    "                      columns=['FT_cbow 0', 'FT_cbow 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_ftcbow_acc = accuracy_score(df_msr_ft['Similarity'], df_msr_ft['fastText_cbow'])\n",
    "\n",
    "msr_ftsg_cf = pd.DataFrame(confusion_matrix(df_msr_ft['Similarity'], df_msr_ft['fastText_sg']),  \n",
    "                      columns=['FT_sg 0', 'FT_sg 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_ftsg_acc = accuracy_score(df_msr_ft['Similarity'], df_msr_ft['fastText_sg'])\n",
    "\n",
    "#sns.heatmap(msr_wdpret_cf, annot=True, cmap='Blues')\n",
    "#plt.show()\n",
    "display(msr_ftcbow_cf)\n",
    "print('Accuracy:', msr_ftcbow_acc)\n",
    "display(msr_ftsg_cf)\n",
    "print('Accuracy:', msr_ftsg_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sent2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "modelsn_train = sent2vec.Sent2vecModel()\n",
    "modelsn_train.load_model('/project/sent2vec/modelsn_train.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "\n",
    "modelsn_unibi = sent2vec.Sent2vecModel()\n",
    "modelsn_unibi.load_model('/project/sent2vec/modelsn_unibi.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sent2vec_model(sentences, model):\n",
    "    \n",
    "    '''Runs Word2Vec and GloVe models in gensim and computes cosine similarity scores for each sentence pair'''\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    for pair in sentences:\n",
    "        embeddings_list = []\n",
    "        for sent in pair:\n",
    "            try:\n",
    "                embeddings_list.append(model.embed_sentence(sent))\n",
    "            except:\n",
    "                embeddings_list.append(np.array(0))\n",
    "                \n",
    "        cos_sim = cosine_similarity(embeddings_list[0].reshape(1, -1), embeddings_list[1].reshape(1, -1))\n",
    "        #cos_sim = dot(sum_list[0], sum_list[1].T)/(linalg.norm(sum_list[0])*linalg.norm(sum_list[1]))\n",
    "        scores.append(float(cos_sim))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_sntrain_scores = run_sent2vec_model(sick_filtered, modelsn_train)\n",
    "msr_sntrain_scores = run_sent2vec_model(msr_filtered, modelsn_train)\n",
    "\n",
    "sick_snunibi_scores = run_sent2vec_model(sick_filtered, modelsn_unibi)\n",
    "msr_snunibi_scores = run_sent2vec_model(msr_filtered, modelsn_unibi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_snunibi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_snunibi_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scores to binary\n",
    "\n",
    "msr_sntrain_binscores = convert_scores_to_binary(msr_sntrain_scores, 0.7)\n",
    "msr_snunibi_binscores = convert_scores_to_binary(msr_snunibi_scores, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_s2v = pd.DataFrame({'Relatedness': sick_scores, 'sent2vec_train': sick_sntrain_scores, \n",
    "                            'sent2vec_unibi': sick_snunibi_scores})\n",
    "df_sick_s2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_s2v.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_s2v.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msr_s2v = pd.DataFrame({'Similarity': msr_scores, 'sent2vec_train': msr_sntrain_binscores, \n",
    "                           'sent2vec_unibi': msr_snunibi_binscores})\n",
    "df_msr_s2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Confusion matrix of fastText scores\n",
    "\n",
    "#plt.figure(figsize=(6, 4))\n",
    "\n",
    "msr_sntrain_cf = pd.DataFrame(confusion_matrix(df_msr_s2v['Similarity'], df_msr_s2v['sent2vec_train']),  \n",
    "                      columns=['S2V_train 0', 'S2V_train 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_sntrain_acc = accuracy_score(df_msr_s2v['Similarity'], df_msr_s2v['sent2vec_train'])\n",
    "\n",
    "msr_snunibi_cf = pd.DataFrame(confusion_matrix(df_msr_s2v['Similarity'], df_msr_s2v['sent2vec_unibi']),  \n",
    "                      columns=['S2V_unibi 0', 'S2V_unibi 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_snunibi_acc = accuracy_score(df_msr_s2v['Similarity'], df_msr_s2v['sent2vec_unibi'])\n",
    "\n",
    "#sns.heatmap(msr_wdpret_cf, annot=True, cmap='Blues')\n",
    "#plt.show()\n",
    "display(msr_sntrain_cf)\n",
    "print('Accuracy:', msr_sntrain_acc)\n",
    "display(msr_snunibi_cf)\n",
    "print('Accuracy:', msr_snunibi_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_docs_list(sentences):\n",
    "    \n",
    "    '''Creates a list of sentences corresponding to a document'''\n",
    "    \n",
    "    docs_list = []\n",
    "    for pair in sentences:\n",
    "        docs_list.append(pair[0])\n",
    "        docs_list.append(pair[1])\n",
    "        \n",
    "    return docs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tagged_documents(sentences):\n",
    "    \n",
    "    '''Gives numerical tags to documents in a list'''\n",
    "    \n",
    "    docs_list = create_docs_list(sentences)\n",
    "    \n",
    "    tagged_docs = []\n",
    "    for i, doc in enumerate(docs_list):\n",
    "        tagged_docs.append(TaggedDocument(words=doc.split(), tags=[i]))\n",
    "    \n",
    "    return docs_list, tagged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_docs_list, sick_tagged_docs = create_tagged_documents(sick_filtered)\n",
    "msr_docs_list, msr_tagged_docs = create_tagged_documents(msr_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_tagged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_tagged_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDocuments(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "    \n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "            for i, line in enumerate(open(os.path.join(self.dirname, fname))):\n",
    "                line = line.translate(translator)\n",
    "                line = line.lower().strip()\n",
    "                yield TaggedDocument(words=line.split(), tags=[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign documents to directory of dataset for training\n",
    "\n",
    "tagged_docs = MyDocuments(\"/project/billion_words_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modeldc_dm = Doc2Vec(tagged_docs, vector_size=300, window=5, min_count=5, workers=20, alpha=0.025, min_alpha=0.025, \n",
    "                     negative=5, dm=1, dbow_words=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_doc2vec_model(tagged_docs, dm, dbow_words):\n",
    "    \n",
    "    '''Train a different Doc2Vec model using specific parameters'''\n",
    "    \n",
    "    model = Doc2Vec(tagged_docs, vector_size=300, window=5, min_count=1, workers=4, alpha=0.025, min_alpha=0.025, \n",
    "                     negative=5, epoch=20, dm=dm, dbow_words=dbow_words)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_modeldc_dm = train_doc2vec_model(sick_tagged_docs, dm=1, dbow_words=0)\n",
    "sick_modeldc_dbow0 = train_doc2vec_model(sick_tagged_docs, dm=0, dbow_words=0)\n",
    "sick_modeldc_dbow1 = train_doc2vec_model(sick_tagged_docs, dm=0, dbow_words=1)\n",
    "\n",
    "msr_modeldc_dm = train_doc2vec_model(msr_tagged_docs, dm=1, dbow_words=0)\n",
    "msr_modeldc_dbow0 = train_doc2vec_model(msr_tagged_docs, dm=0, dbow_words=0)\n",
    "msr_modeldc_dbow1 = train_doc2vec_model(msr_tagged_docs, dm=0, dbow_words=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sick_modeldc_dm.most_similar('kid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msr_modeldc_dm.most_similar('kid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sick_modeldc_dm.infer_vector(sick_docs_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_doc2vec_model(docs_list, model):\n",
    "    \n",
    "    '''Runs Doc2Vec models and computes cosine similarity scores for each sentence (document) pair'''\n",
    "    \n",
    "    scores = []\n",
    "\n",
    "    i = 0\n",
    "    while i < len(docs_list):\n",
    "\n",
    "        vector1 = model.infer_vector(docs_list[i])\n",
    "        vector2 = model.infer_vector(docs_list[i+1])\n",
    "        \n",
    "        cos_sim = cosine_similarity(vector1.reshape(1, -1), vector2.reshape(1, -1))\n",
    "        #cos_sim = dot(vector1.T, vector2)/(linalg.norm(vector1)*linalg.norm(vector2))\n",
    "        #print(cos_sim)\n",
    "        scores.append(float(cos_sim))\n",
    "        i += 2\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation\n",
    "\n",
    "sick_dcdm_scores = run_doc2vec_model(sick_docs_list, sick_modeldc_dm)\n",
    "sick_dcdbow0_scores = run_doc2vec_model(sick_docs_list, sick_modeldc_dbow0)\n",
    "sick_dcdbow1_scores = run_doc2vec_model(sick_docs_list, sick_modeldc_dbow1)\n",
    "\n",
    "msr_dcdm_scores = run_doc2vec_model(msr_docs_list, msr_modeldc_dm)\n",
    "msr_dcdbow0_scores = run_doc2vec_model(msr_docs_list, msr_modeldc_dbow0)\n",
    "msr_dcdbow1_scores = run_doc2vec_model(msr_docs_list, msr_modeldc_dbow1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert scores to binary\n",
    "\n",
    "msr_dcdm_binscores = convert_scores_to_binary(msr_dcdm_scores, 0.7)\n",
    "msr_dcdbow0_binscores = convert_scores_to_binary(msr_dcdbow0_scores, 0.7)\n",
    "msr_dcdbow1_binscores = convert_scores_to_binary(msr_dcdbow1_scores, 0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sick_dcdm_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_d2v = pd.DataFrame({'Relatedness': sick_scores, 'Doc2Vec_dm': sick_dcdm_scores, 'Doc2Vec_dbow0': sick_dcdbow0_scores,\n",
    "                              'Doc2Vec_dbow1': sick_dcdbow1_scores})\n",
    "df_sick_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_d2v.corr(method='pearson')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sick_d2v.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_msr_d2v = pd.DataFrame({'Similarity': msr_scores, 'Doc2Vec_dm': msr_dcdm_binscores, 'Doc2Vec_dbow0': msr_dcdbow0_binscores,\n",
    "                              'Doc2Vec_dbow1': msr_dcdbow1_binscores})\n",
    "df_msr_d2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix of Doc2Vec scores\n",
    "\n",
    "#plt.figure(figsize=(6, 4))\n",
    "\n",
    "msr_dcdm_cf = pd.DataFrame(confusion_matrix(df_msr_d2v['Similarity'], df_msr_d2v['Doc2Vec_dm']),  \n",
    "                      columns=['D2V_dm 0', 'D2V_dm 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_dcdbow0_cf = pd.DataFrame(confusion_matrix(df_msr_d2v['Similarity'], df_msr_d2v['Doc2Vec_dbow0']),  \n",
    "                      columns=['D2V_dbow0 0', 'D2V_dbow0 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "msr_dcdbow1_cf = pd.DataFrame(confusion_matrix(df_msr_d2v['Similarity'], df_msr_d2v['Doc2Vec_dbow1']),  \n",
    "                      columns=['D2V_dbow1 0', 'D2V_dbow1 1'], index=['MSR 0', 'MSR 1'])\n",
    "\n",
    "#sns.heatmap(msr_dcdm_cf, annot=True, cmap='Blues')\n",
    "#plt.show()\n",
    "print(msr_dcdm_cf, '\\n')\n",
    "print(msr_dcdbow0_cf, '\\n')\n",
    "print(msr_dcdbow1_cf, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Python3]",
   "language": "python",
   "name": "conda-env-Python3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
